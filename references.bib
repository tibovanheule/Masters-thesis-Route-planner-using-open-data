
@misc{noauthor_otp_2023,
	title = {{OTP} {Summit}},
	url = {https://groups.google.com/g/opentripplanner-dev/c/R8g9I1kId_4/m/mc8y0y1ZAwAJ?pli=1},
	month = nov,
	year = {2023},
	keywords = {Open Trip Planner, Raptor, server side},
}

@misc{noauthor_raptor_2023,
	title = {Raptor implementation},
	url = {https://github.com/planarnetwork/raptor},
	month = nov,
	year = {2023},
	note = {Publication Title: GitHub},
}

@article{delling_round-based_2015,
	title = {Round-{Based} {Public} {Transit} {Routing}},
	volume = {49},
	issn = {0041-1655},
	url = {https://pubsonline.informs.org/doi/10.1287/trsc.2014.0534},
	doi = {10.1287/trsc.2014.0534},
	abstract = {We study the problem of computing all Pareto-optimal journeys in a dynamic public transit network for multiple criteria, such as arrival time and number of transfers. Existing algorithms consider this as a graph problem and solve it using various graph search algorithms. Unfortunately, this leads to either high query times or suboptimal solutions. We take a different approach. We introduce RAPTOR, our novel round-based public transit router. Unlike previous algorithms, it is not Dijkstra-based, looks at each route (such as a bus line) in the network at most once per round, and can be made even faster with simple pruning rules and parallelization using multiple cores. Because it does not rely on preprocessing, RAPTOR works in fully dynamic scenarios. Starting from arrival time and number of transfers as criteria, it can be easily extended to handle flexible departure times or arbitrary additional criteria. As practical examples we consider fare zones and reliability of transfers. When run on complex public transportation networks (such as London), RAPTOR computes all Pareto-optimal journeys between two random locations an order of magnitude faster than previous approaches, which easily enables interactive applications.},
	number = {3},
	urldate = {2024-01-20},
	journal = {Transportation Science},
	author = {Delling, Daniel and Pajor, Thomas and Werneck, Renato F.},
	month = aug,
	year = {2015},
	note = {Publisher: INFORMS},
	keywords = {dynamic programming, multicore, multicriteria optimization, public transit, shortest paths, timetable information},
	pages = {591--604},
}

@inproceedings{dibbelt_intriguingly_2013,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Intriguingly {Simple} and {Fast} {Transit} {Routing}},
	isbn = {978-3-642-38527-8},
	doi = {10.1007/978-3-642-38527-8_6},
	abstract = {This paper studies the problem of computing optimal journeys in dynamic public transit networks. We introduce a novel algorithmic framework, called Connection Scan Algorithm (CSA), to compute journeys. It organizes data as a single array of connections, which it scans once per query. Despite its simplicity, our algorithm is very versatile. We use it to solve earliest arrival and multi-criteria profile queries. Moreover, we extend it to handle the minimum expected arrival time (MEAT) problem, which incorporates stochastic delays on the vehicles and asks for a set of (alternative) journeys that in its entirety minimizes the user’s expected arrival time at the destination. Our experiments on the dense metropolitan network of London show that CSA computes MEAT queries, our most complex scenario, in 272 ms on average.},
	language = {en},
	booktitle = {Experimental {Algorithms}},
	publisher = {Springer},
	author = {Dibbelt, Julian and Pajor, Thomas and Strasser, Ben and Wagner, Dorothea},
	editor = {Bonifaci, Vincenzo and Demetrescu, Camil and Marchetti-Spaccamela, Alberto},
	year = {2013},
	keywords = {Arrival Time, Early Arrival, Priority Queue, Stochastic Delay, Transit Route},
	pages = {43--54},
}

@inproceedings{bast_fast_2010,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Fast {Routing} in {Very} {Large} {Public} {Transportation} {Networks} {Using} {Transfer} {Patterns}},
	isbn = {978-3-642-15775-2},
	doi = {10.1007/978-3-642-15775-2_25},
	abstract = {We show how to route on very large public transportation networks (up to half a billion arcs) with average query times of a few milliseconds. We take into account many realistic features like: traffic days, walking between stations, queries between geographic locations instead of a source and a target station, and multi-criteria cost functions. Our algorithm is based on two key observations: (1) many shortest paths share the same transfer pattern, i.e., the sequence of stations where a change of vehicle occurs; (2) direct connections without change of vehicle can be looked up quickly. We precompute the respective data; in practice, this can be done in time linear in the network size, at the expense of a small fraction of non-optimal results. We have accelerated public transportation routing on Google Maps with a system based on our ideas. We report experimental results for three data sets of various kinds and sizes.},
	language = {en},
	booktitle = {Algorithms – {ESA} 2010},
	publisher = {Springer},
	author = {Bast, Hannah and Carlsson, Erik and Eigenwillig, Arno and Geisberger, Robert and Harrelson, Chris and Raychev, Veselin and Viger, Fabien},
	editor = {de Berg, Mark and Meyer, Ulrich},
	year = {2010},
	keywords = {Local Search, Optimal Cost, Query Time, Target Station, Transfer Pattern, transfer patterns},
	pages = {290--301},
}

@misc{noauthor_9th_2017,
	title = {9th {DIMACS} {Implementation} {Challenge}: {Shortest} {Paths}},
	shorttitle = {9th {DIMACS} {Implementation} {Challenge}},
	url = {https://web.archive.org/web/20170606011114/http://www.dis.uniroma1.it/challenge9/format.shtml},
	urldate = {2024-01-20},
	month = jun,
	year = {2017},
}

@incollection{bast_car_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Car or {Public} {Transport}—{Two} {Worlds}},
	isbn = {978-3-642-03456-5},
	url = {https://doi.org/10.1007/978-3-642-03456-5_24},
	abstract = {There are two kinds of people: those who travel by car, and those who use public transport. The topic of this article is to show that the algorithmic problem of computing the fastest way to get from A to B is also surprisingly different on road networks than on public transportation networks.},
	language = {en},
	urldate = {2024-01-20},
	booktitle = {Efficient {Algorithms}: {Essays} {Dedicated} to {Kurt} {Mehlhorn} on the {Occasion} of {His} 60th {Birthday}},
	publisher = {Springer},
	author = {Bast, Hannah},
	editor = {Albers, Susanne and Alt, Helmut and Näher, Stefan},
	year = {2009},
	doi = {10.1007/978-3-642-03456-5_24},
	keywords = {Local Search, Public Transport, Road Network, Road Segment, Short Path},
	pages = {355--367},
}

@misc{noauthor_rdf_2023,
	title = {{RDF} {Mapping} {Language} ({RML})},
	url = {https://rml.io/specs/rml},
	month = sep,
	year = {2023},
}

@inproceedings{houda_public_2010,
	title = {A public transportation ontology to support user travel planning},
	doi = {10.1109/RCIS.2010.5507372},
	booktitle = {2010 {Fourth} {International} {Conference} on {Research} {Challenges} in {Information} {Science} ({RCIS})},
	author = {Houda, Mnasser and Khemaja, Maha and Oliveira, Kathia and Abed, Mourad},
	year = {2010},
	pages = {127--136},
}

@article{bouhana_ontology-based_2015,
	title = {An ontology-based {CBR} approach for personalized itinerary search systems for sustainable urban freight transport},
	volume = {42},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417414007908},
	doi = {https://doi.org/10.1016/j.eswa.2014.12.012},
	abstract = {This paper presents a novel information retrieval approach for personalized itinerary search in urban freight transport systems. The proposed approach is based on the integration of three techniques: Case Base Reasoning, Choquet integral and ontology. It has the following advanced features: (1) user-oriented ontology is used as source of knowledge to extract pertinent information about stakeholder’s preferences and needs; (2) semantic web rule language is considered to provide the system with enhanced semantic capabilities and support personalized case representation; (3) a CBR-personalized retrieval mechanism is designed to provide a user with an optimum itinerary that meets his personal needs and preferences. The above features lead to a personalized and optimum itinerary search that meets the user’s needs as specified in their queries such as fuel consumption, environmental impact, optimum route, time management etc. This has the potential to effectively manage fright movement according to stakeholder’s needs and alleviate congestion problems in urban areas. The proposed intelligent decision support system (Onto-CBR) is implemented to an itinerary search problem for freight transportation users in urban areas. Its performance is further compared to an itineraries search system that was proposed by the authors in an earlier publication. Both approaches are compared in terms of their ability to meet user’s personal preferences and achieve accuracy in case retrieval. The experimental results showed the ability of the proposed system to improve the accuracy of case retrieval and reduce retrieval time prominently. The ability of the proposed system tailor the search to stakeholders needs, improve the accuracy of case retrieval and facilitate the search process are among the main positive features of the proposed intelligent decision support system.},
	number = {7},
	journal = {Expert Systems with Applications},
	author = {Bouhana, Amna and Zidi, Amir and Fekih, Afef and Chabchoub, Habib and Abed, Mourad},
	year = {2015},
	keywords = {Information retrieval, Itinerary search, Multi-criteria decision support, Personalization, Urban freight transport},
	pages = {3724--3741},
}

@misc{noauthor_oslo_2023,
	title = {{OSLO} {Mobiliteit} - {Dienstregeling} en {Planning}: {Tijdstabellen} ({Applicatieprofiel})},
	url = {https://data.vlaanderen.be/doc/applicatieprofiel/mobiliteit/dienstregeling-en-planning/tijdstabellen},
	month = jul,
	year = {2023},
}

@article{gruber_translation_1993,
	title = {A translation approach to portable ontology specifications},
	volume = {5},
	issn = {1042-8143},
	url = {https://www.sciencedirect.com/science/article/pii/S1042814383710083},
	doi = {https://doi.org/10.1006/knac.1993.1008},
	abstract = {To support the sharing and reuse of formally represented knowledge among AI systems, it is useful to define the common vocabulary in which shared knowledge is represented. A specification of a representational vocabulary for a shared domain of discourse—definitions of classes, relations, functions, and other objects—is called an ontology. This paper describes a mechanism for defining ontologies that are portable over representation systems. Definitions written in a standard format for predicate calculus are translated by a system called Ontolingua into specialized representations, including frame-based systems as well as relational languages. This allows researchers to share and reuse ontologies, while retaining the computational benefits of specialized implementations. We discuss how the translation approach to portability addresses several technical problems. One problem is how to accommodate the stylistic and organizational differences among representations while preserving declarative content. Another is how to translate from a very expressive language into restricted languages, remaining system-independent while preserving the computational efficiency of implemented systems. We describe how these problems are addressed by basing Ontolingua itself on an ontology of domain-independent, representational idioms.},
	number = {2},
	journal = {Knowledge Acquisition},
	author = {Gruber, Thomas R.},
	year = {1993},
	pages = {199--220},
}

@misc{noauthor_gtfs_2022,
	title = {{GTFS} {Static} {Overview}},
	url = {https://developers.google.com/transit/gtfs},
	month = sep,
	year = {2022},
	note = {Publication Title: Google for Developers},
}

@misc{noauthor_last-modified_2023,
	title = {Last-{Modified} - {HTTP} {\textbackslash}vert {MDN}},
	url = {https://developer.mozilla.org/fr/docs/Web/HTTP/Headers/Last-Modified},
	month = nov,
	year = {2023},
}

@misc{noauthor_gtfsbe_2022,
	title = {{GTFS}.be - {GTFS} {Belgium}},
	url = {https://gtfs.be/index_en.html},
	month = dec,
	year = {2022},
}

@article{flipts_fragmenting_2020,
	title = {Fragmenting public transport timetables on the web in tiles for serverless route planning},
	url = {https://lib.ugent.be/en/catalog/rug01:002945734?i=0&q=Fragmenting+public+transport+timetables+on+the+web+in+tiles+for+serverless+route+planning+},
	journal = {2020.},
	author = {Flipts, Jeroen},
	year = {2020},
	note = {Publisher: 2020.},
}

@inproceedings{rojas_melendez_julian_andres_decentralized_2020,
	title = {Decentralized route planning across the web of data},
	volume = {2548},
	url = {{http://ceur-ws.org/Vol-2548/paper-09.pdf}},
	abstract = {Wheelchair users looking for accessible public transport routes, tourists discovering attractive routes to go around a new city, or bicycle users trying to avoid highly polluted routes are some examples where highly individualized route planning is needed. Current route planning applications lack query flexibility. The types of queries supported by a route planner are only determined at design time and heavily depend on centralized pre-selected data sources. Integrating a new data source such as another transport mode, a different road network or wheelchair accessibility is not straightforward as it generally requires human intervention to extend the subjacent data model and route planning algorithm implementation. I investigate how relevant data sources available on the Web can be dynamically reused for answering custom queries, and thus allow creating more flexible and personalized route planning applications. Semantic Web and Linked Data technologies provide a common framework for data integration. Yet it is still unclear how relevant data can be automatically reused while remaining independent from specific route planning algorithm implementations. Preliminary work (i) tests the feasibility of solving route planning queries over live and static public transport data sources on the Web, (ii) explores the trade-offs of different Web APIs for publishing and consuming live data streams on the Web and (iii) introduces a Linked Data based approach for publishing road networks data.},
	language = {eng},
	booktitle = {Proceedings of the {Doctoral} {Consortium} at {ISWC} 2019 ({ISWC}-{DC} 2019)},
	author = {{Rojas Melendez, Julian Andres}},
	year = {2020},
	note = {ISSN: 1613-0073
Place: Auckland, New Zealand},
	keywords = {Knowledge Graph, Linked Data, Route Planning, Web API},
	pages = {93--103},
}

@article{katsumi_ontologies_2018,
	title = {Ontologies for transportation research: {A} survey},
	volume = {89},
	issn = {0968-090X},
	doi = {10.1016/j.trc.2018.01.023},
	journal = {Transportation Research Part C: Emerging Technologies},
	author = {Katsumi, Megan and Fox, Mark},
	month = apr,
	year = {2018},
	note = {Publisher: Pergamon},
	pages = {53--82},
}

@article{noauthor_movidius_2015,
	title = {The {Movidius} {Myriad} {Architecture}'s {Potential} for {Scientific} {Computing} - {IEEE} {Journals} \& {Magazine}},
	issn = {Information:},
	url = {https://ieeexplore.ieee.org/document/7006377},
	abstract = {Document Sections1.The Myriad I Mobile Processor2.Matrix Multiplication on a Single Core3.Multicore AlgorithmAuthorsFiguresReferencesCitationsKeywordsMetricsMore Like This Download PDF Download Citation View References Email Request Permissions Export to Collabratec Alerts Abstract: In recent years, a new generation of ultralow-power processors have emerged that are aimed primarily at signal processing in mobile computing. However, their architecture... View more Metadata Abstract:In recent years, a new generation of ultralow-power processors have emerged that are aimed primarily at signal processing in mobile computing. However, their architecture could make some of these useful for other applications. Algorithms originally developed for scientific computing are used increasingly in signal conditioning and emerging fields such as computer vision, increasing the demand for computing power in mobile systems. In this article, the authors describe the design and implementation of dense matrix multiplication on the Movidius Myriad architecture and evaluate its performance and energy efficiency. The authors demonstrate a performance of 8.11 Gflops on the Myriad I processor and a performance/watt ratio of 23.17 Gflops/W for a key computational kernel. These results show significant potential for scientific-computing tasks and invite further research.Published in: IEEE Micro ( Volume: 35 , Issue: 1 , Jan.-Feb. 2015 ) Page(s): 6 - 14Date of Publication: 12 January 2015 ISSN Information: INSPEC Accession Number: 14982516 DOI: 10.1109/MM.2015.4Publisher: IEEEAdvertisement Contents This Article Describes The Design And Implementation Of Dense Matrix Multiplication On The Movidius Myriad Architecture And Evaluates Its Performance And Energy Efficiency. The Authors Demonstrate A Performance Of 8.11 Gflops On The Myriad I Processor, And A Performance/watt Ratio Of 23.17 Gflops/w For A Key Computational Kernel. These Results Show Significant Potential For Scientific-computing Tasks And Invite Further Research.â¦ â¦ Over the past decade, power and energy efficiency have become two of the main limiters of parallel-computing performance from the perspectives of absolute energy efficiency and thermal power dissipation, with mobile computing being the major driving factor behind this trend. However, the concern with energy efficiency is not limited to mobile computing; a system's energy efficiency (in the form of performance per watt) has become a key metric. CPUs, digital signal processors, and field-programmable gate arrays are typically significantly more energy efficient than conventional desktop or server CPUs; their deployment often leads to 4x increases in energy efficiency over traditional CPUs.1,2 With mobile processor architectures becoming ever more versatile, the market is gradually moving away from custom silicon solutions toward energy-efficient CPUs.The Movidius Myriad I processor can achieve 15.84 Cflops while dissipating only 0.35 W,3 for a theoretical peak of around 45 Gflops/W This level of power efficiency results from fundamental architectural choices: very long instruction word (VLIW), instruction-level parallelism (ILP), noninter-locked execution pipelines, and software managed on chip memory. However, the design choices leading to that level of performance make it an interesting target for a broader family of applications.We investigate the possibility of using this architecture for typical scientific workloads; more specifically, we implement and evaluate the SGEMM primitive (dense matrix multiplication) on this platform. SGEMM is part of the Basic Linear Algebra Subroutines (BLAS) Level 3 family of functions, and is regarded as a major indicator of a system's computational potential.4In order to achieve an efficient implementation, we exploit the platform's multiple kinds of parallelism, and we provide extensive details of the process, from which an application design methodology can be derived.This article describes the design and implementation of SGEMM on the Myriad I mobile processor and shows how to use the Myriad instruction set to maximize single-core performance. We also describe our explicit data movement algorithm, which exploits data locality in the banked local memory, and we evaluate our implementation's performance in terms of both computational throughput and energy efficiency.The Myriad I Mobile ProcessorMyriad I is a mobile processor designed from the ground up for power-efficient computation. It features eight highly specialized cores featuring single-instruction, multiple-data (SIMD) processing and instruction predication, known as streaming hybrid architecture vector engines (SHAVEs). A general-purpose reduced-instruction-set computing (RISC) core is used for task allocation, coordination, and peripherals management (Figure 1).Figure 1. Movidius Myriad I processor, top-level view. The interconnect fabric is shown, as are the core components: the memory (CMX, DDR, L2 cache), the cores (SHAVEs, RISC), and the SHAVE control interface (SCTL).View AllThe chip's main memory comprises either 64 or 16 Mbytes of DRAM; it resides inside the CPU package on a separate die and is bonded internally to the main Myriad die. Additionally, the main die contains 1 Mbyte of directly addressable static RAM (SRAM), organized as eight physical 128- Kbyte slices, each associated with one SHAVE core. This SRAM is arranged in a connection matrix (CMX) topology, which forwards accesses between the slices. It is not a bus slave; instead, each SHAVE has dedicated memory access ports in its (physically) neighboring memory slice, which is described as being local to it. A SHAVE core always posts a CMX memory request to its local slice, which either services it or forwards it to the target slice through the CMX. This lets any SHAVE access any CMX slice at any time.The CMX memory comprises single-port SRAM blocks; therefore, it has the performance of an L1 cache while being completely addressable. This makes the CMX the preferred operating memory for both code and data.Each SHAVE core has a direct memory access (DMA) engine for moving data between the CMX and main memory. This data movement is entirely under software control and requires the programmer to explicitly request the transfers.The system features 128 Kbytes of L2 cache, which caches only the main memory, and can be used as a shared cache or can be partitioned into multiple independent caches.A SHAVE core has a VLIW architecture that can start up to eight operations per cycle. It features two independent load/store units, a branch/repeat unit, a predicated execution unit, and three arithmetic unitsâinteger, scalar, and vector. Additionally, a data comparison and moving unit handles data comparisons, data copying between register files, and format conversions. Every instruction can issue operations to any combination of the execution units.Each of these units can begin executing one operation per cycle, which can lead to significant ILP. However, no data dependency checking is performed in hardware; it is entirely up to the compiler (or programmer) to find operations that can be issued in parallel, and to form instructions byscheduling these together.The SHAVE cores fully implement single-precision IEEE754 floating-point arith-metic, as well as OpenCL half-precision (16-bit) arithmetic. The RISC core also features double-precision floating-point arithmetic, making mixed-precision algorithms feasible.The Myriad I chip can run at a top frequency of 200 MHz, with its memory clocked no higher than 133 MHz.Concerning power management, Myriad favors running at the maximum required frequency at all times, while allowing power to unused parts of the chip to be shut down dynamically. Given the chip's low thermal design power (TDP) and the significant contribution of static dissipation to this TDP, the frequency power correlation is rather weak, as predicted by previous research.5,6Matrix Multiplication on a Single CoreIn order to achieve good performance, two things are required: an efficient, single-core kernel and a suitable scheduling algo-ri hm. This section describes the design of the kernel, which will be replicated on all the cores. Naive vector parallelismOur matrix multiplication algorithm is built around a kernel that computes a single 4 Ã 4 block of the result matrix C, by multiplying a 4 Ã 4 block of each of the two input matrices, A and B, We use vector SIMD operations to implement it, and keep each of the 4 Ã 4 blocks of A,B, and C in registers; each 4 Ã 4 block requires four vector registers, so all blocks fit comfortably in the existing 32 registers.Figure 2 shows the pseudocode for a simplified version of the vector intrinsic code in our kernel. Note that we store the matrices in C (row major) format.Figure 2. Kernel pseudocode fragment for matrix multiplication. Depicted is the code that reads the data from memory, as well as the code that computes one row of the result; the other three steps are identical. C matrix reading/writing happens before and after the kernel, rather than in it, and is not depicted.View AllThe algorithm comprises two loops:The inner loop walks simultaneously over four rows Of A and four columns of B fully computing a 4 Ã 4 C block.An outer loop walks across all of B and A running the inner loop on each pair of blocks.To compute a 4 Ã 4 cell of C we must multiply the rows of A by the columns of B so in order to use SIMD, the B block must be transposed. The Myriad SHAVE supports a single-cycle 4 Ã 4 transpose instruction, which avoids the cost of vector shuffling; the pseudocode uses a single intrinsic to show that.Multiplying two 4 Ã 4 blocks requires 128 floating-point operations (64 multiplications, 64 additions). The vector arithmetic unit (VAU) can start a vector multiply or addition per cycle, operating on four pairs of values, so by using the VAU we could expect the kernel to take at least 32 cycles.However, the scalar arithmetic unit (SAU) features a horizontal-vector sum operation. It is therefore possible to initiate both a vector multiplication and a horizontal sum in the same machine cycle, reducing the problem of implementing an efficient 4 Ã 4 block matrix multiplication to performing the computation using vector multiplies and horizontal sums.The second half of Figure 2 shows our solution: we multiply a row of A by a row of BT (a column of B), and we then use the horizontal sum to reduce the four products to the value of one element of the resulting C block.The kernel accumulates the values of successive 4 Ã 4 matrix multiplications to a single C block, which we represent as four vectors. To do this, we gather the sums from the scalar registers where they are produced into a vector register (this\_c): once this\_c is populated, it is added to the vectors representing the C block. Instruction-Level ParallelismIn addition to vector parallelism, the SHAVE cores offer significant ILP, allowing multiple operations to be executed each cycle. In order to overlap the execution of operations from successive iterations of our kernel loop, we applied modulo scheduling.7 This allows a new iteration of the loop to begin before the previous one completes.Using VAU multiplication operations and SAU horizontal summations, we need only 16 instructions to perform all 64 required multiplications and 48 of the 64 required additions. The remaining 16 additions can be performed using VAU (vertical) additions-accumulating the current result to the C buffer.The VAU is thus the critical resource, being busy for at least 20 cycles in any instruction schedule, performing the multiplications and the C accumulation. Platform-Specific ImprovementsTo lower the required instruction count further, an alternative approach was sought. First, the SHAVE cores feature a SIMD multiply and accumulate unit, with a dedicated vector accumulator. Second, the SAU is idle much of the time, and some of the result accumulation can be performed on it. These changes complicate the loop, but with aggressive modulo scheduling, it can be compacted into 18 instructions, with an ILP of 3.39.An iteration of the kernel runs in 19 cycle . Given that each iteration performs 128 floating-point operations, the kernel achieves a performance of 6.74 floating-point operations (flops) per cycle. The measured performance of the entire algorithm is 6.62 flops/cycle for CMX slice-sized workloads.Multicore AlgorithmMyriad I does not have a threading system for executing code on the SHAVE cores; all the memory is shared, leaving it to the programmer to choose how best to use it. In these circumstances, we chose to program the Myriad as a bare metal system, allowing the code to fully control its behavior. We chose to implement all the scheduling code on the RISC core, and use the SHAVE cores only for computation. BackgroundWe found the major system bottleneck to be the main memory interface. In particular, using the same kernel on a 1-Mbyte dataset, we achieved 6.62 flops/cycle (in CMX) versus 0.14 flops/cycle (in DRAM)âa 47Ã decrease in performance. This difference prompted us to discard from the onset the possibility of using in-place DRAM buffers. Data Movement and Locality ConsiderationsThere is a total of 1 Mbyte of CMX memory, divided into eight 128-Kbyte slices, each paired to a specific SHAVE core (âlocal sliceâ). However, any SHAVE core can access any memory location in any of the CMX slices.This design is rather novel; the existing literature on programming systems with per-core isolated memory (such as the Cell processor2) does apply, but it doesn't address the possibilities opened by the CMX slices not being isolated. Furthermore, every SHAVE core has its own DMA engine, which operates in parallel with the regular instruction execution. This fact immediately points to the potential usefulness of double buffering.To operate on larger-than-CMX-memory buffers, we used tiling to divide the matrices into blocks that fit the locally available CMX memory. On each SHAVE, we programmed its DMA controller to transfer the required blocks (A,B and the previous C) into the local buffers, performed the computation locally, and then used the DMA controller to transfer the result (updated C block) back to its main memory destination. Also, because DRAM accesses are expensive performance-wise, even when using DMA transfers, it is desirable to reuse data already available in CMX, where possible. Multicore ImplementationsOur first implementation attempted to maximize the cached data reuse, as in Kodukula et al.8 However, instead of minding caches, our implementation used a DMA schedule-blocks of A and B get âcachedâ in the CMX memory, matrix multiplication is performed locally, and the result is copied back to the main memory (see Figure 3).Figure 3. Basic CMX tiling: one (i,j) iteration.View AllLet the matrices be. A:mÃk,B:kÃn, and C:mÃn. We define mâ²(â¤m),kâ²(â¤k), and nâ²(â¤n), such that a SHAVE core's local CMX memory can accommodate the buffers a:mâ²Ãkâ²,b:kâ²Ãnâ², and c:mâ²Ãnâ². The \{mâ²,kâ²,nâ²\} triplet denotes the matrix block sizes.A scheduler runs on the RISC core, dividing the matrices into \{mâ²,kâ²,nâ²\} blocks that are assigned to SHAVE cores for computation; the code on the SHAVE will DMA the input data to CMX, compute locally, and DMA the result to DRAM.This approach resulted in a performance of up to 11.71 flops/cycle. The follow-up investigation showed that the performance was severely limited by the L2 cache thrashing; the obvious solution would be to maximize cache reuse, giving priority to B which is not accessed sequentially. We modified the algorithm by allocating a maximally sized B buffer in each slice, and transferring the same B block to all SHAVEs.To determine the matrix block sizes, we use the following algorithm:find the largest kâ² such that a CMX slice has room for a (4Ãkâ²)A block in any round. Additionally, we chose a slice memory layout that minimizes the chance of A/B RAM access conflicts.The algorithm proceeds as follows:DMA individual blocks of B,A and C, respectively, to all the SHAVEs.Let i=0; for each SHAVE select output buffer ci and run all of them on their local A buffer, and the ith (modulo nShaves) next SHAVE's B buffer, storing the results in the local ci buffer; proceed to the next step only after all are finished computing (see Figure 5a).Iterate at Step 2 for i=1â¦nShavesâ1 (Figure 5b depicts Step 2 when i=1).Here, all A and B buffer combinations have been processed, and every SHAVE's (c0â¦nShavesâ1) buffers are full.DMA all the c buffers, from all CMX slices, back to DRAM.For each SHAVE, in sequence, load only the next A horizontal blocks, and iterate at Step 2.Once no more A blocks are available, DMA again the first nShaves blocks of A and the next nShaves blocks of B, and iterate at Step 2.Once no more B blocks are available, the algorithm is complete.We achieved 37.43 flops/cycle, a significant improvement over the classical approach. Also, because every SHAVE core receives its unique block of A and B double buffering is likely to increase the performance further. Since B blocks change nShaves less often than A or C blocks, we chose to use double buffering only for the latter. We achieved this by modifying the blocking strategy to allow for two A buffers (a0â¦1:mâ²Ãkâ²), several C buffers (c0â¦1,0â¦nShavesâ1:mâ²Ãnâ²), and a B buffer (b:kâ²Ãnâ²). The algorithm itself flows as described previously, with one change: A and C refills and C flushes happen in the background.Figure 5. Step 2 of the algorithm: (a) when i=0; (b) when i=1. It shows how every SHAVE's B and C buffers change depending on the iteration, while the A buffer is retained at all times.View AllTable 1. Estimated performance of existing architectures for SGEMM.Double buffering improves performance to 45.05 flops/cycle, equivalent to 5.63 flops/ cycle per core. Considering that the kernel's peak performance is 6.62 flops/cycle per core, we are achieving 85 percent of the peak performance for SGEMM. Given the system frequency of 180 MHz, this translates to a performance of 8.11 Gflops. Energy efficiencyOur experimental results show that our matrix multiplication on eight cores offers overall performance of 8.11 Gflops, and the published TDP for Myriad I is 0.35 W3 On the basis of these figures, we can expect that Myriad I will achieve 23.17 Gflops/WTable 1 summarizes the energy efficiency of several general-purpose and special-purpose processors when running SGEMM. These figures are estimates compiled by Igual et al.9 and Pedram et al.,10 as well as data presented by Intel11 and Nvidia12; although these estimates might not be perfect, they have been used in a recent similar study of a digital signal processor.9Because SGEMM is computationally bound, we expect that for most of these platforms the systems would run at full speed, regardless of whether they could be more efficient at a different frequency or voltage point. Additionally, it makes us expect that the energy efficiency of most of these systems would peak alongside the maximum performance.5 DiscussionThe energy efficiency achieved by Myriad I for SGEMM is remarkable, considering it is exceeded only by that of a 28-nm GPU, whereas Myriad is a 65-nm processor. A single algorithm is insufficient to draw overall conclusions; however, comparing its architecture with existing systems could let us infer more about its potential.The most similar existing top-level architecture is that of Cell, the performance profile of which is well-known. There are, however, outstanding differences. Synergistic processing elements (SPEs) are purely vector cores, whereas the SHAVEs are more similar to bundles of specialized RISC cores. SHAVEs have scalar registers, and the memory is granular at byte level. Also, their instruction encoding permits twice the ILP possible on an SPE. Furthermore, the memory model is more flexible than that of Cell, which lacks a unified address space.We can draw two conclusions. First, the similarity of Myriad and Cell leads to the expectation that, maintaining proportions, their performance curves would be largely sim lar for the same application. Second, the SHAVE design lets us leverage the developments in VLIW compiler research alongside those in the RISC field.A major issue concerning scientific computing is the lack of double-precision floating-point support on SHAVEs. We can partially avoid this by using mixed-precision algorithms, running the bulk of the computation on the SHAVEs and the refinement step on the RISC, which is double-precision capable. In this respect, we believe Myriad is in a similar position to GPUs of 10 to 15 years ago: if the potential were demonstrated, hardware support would follow.Scientific-computing techniques have important applications outside the traditional scientific establishment. Linear algebra is used in game physics, computer vision, and other applications that were once confined to large-scale computing but are now increasingly found in mobile systems. As computing moves onto mobile devices, the need for energy-efficient high-performance platforms will only increase.In this field, the Myriad architecture is unique, mainly because it was designed from scratch with the goal of providing specifically that. We believe that Myriad represents a new class of ultra low-power processors; this article aims to show the Myriad architecture's potential for scientific computing, so that parallel-computing researchers can start to consider these types of processors in future machines. However, as with GPUs a decade ago, some practical problems remain to be solved. Nonetheless, history suggests that many of the most important lessons are learned at the boundaries of the technology, and that hardware and software techniques developed for the most power-efficient systems will influence other systems where power efficiency is important.In this article, we have focused on Myriad's architectural features. However, given their benefits for both power efficiency and computational throughput, we believe that some of these features will become mainstream in the next generations of computing platforms. MICROACKNOWLEDGMENTThis work was supported in part by Science Foundation Ireland grants 10/CE/ I1855 and 12/IA/1381 to Lero, the Irish Software Engineering Research Centre (www.lero.ie). We also thank Movidius for the continued support that made this work possible. AuthorsFiguresReferencesCitationsKeywordsMetrics},
	year = {2015},
	keywords = {Metrics},
}

@article{noauthor_statistical_2012,
	title = {Statistical performance comparisons of computers - {IEEE} {Conference} {Publication}},
	issn = {Information:},
	url = {https://ieeexplore.ieee.org/document/6169043},
	abstract = {Document Sections1Introduction2Motivation3Issues with Current Performance Comparison Practices4Non-parametric Hierarchical Performance Testing Framework5Experimental ComparisonsAuthorsFiguresReferencesCitationsKeywordsMetricsMore Like ThisFootnotes Download PDF Download Citation View References Email Request Permissions Export to Collabratec Alerts Abstract: As a fundamental task in computer architecture research, performance comparison has been continuously hampered by the variability of computer performance. In traditional ... View more Metadata Abstract:As a fundamental task in computer architecture research, performance comparison has been continuously hampered by the variability of computer performance. In traditional performance comparisons, the impact of performance variability is usually ignored (i.e., the means of performance measurements are compared regardless of the variability), or in the few cases where it is factored in using parametric confidence techniques, the confidence is either erroneously computed based on the distribution of performance measurements (with the implicit assumption that it obeys the normal law), instead of the distribution of sample mean of performance measurements, or too few measurements are considered for the distribution of sample mean to be normal. We first illustrate how such erroneous practices can lead to incorrect comparisons. Then, we propose a non-parametric Hierarchical Performance Testing (HPT) framework for performance comparison, which is significantly more practical than standard parametric techniques because it does not require to collect a large number of measurements in order to achieve a normal distribution of the sample mean. This HPT framework has been implemented as an open-source software.Published in: IEEE International Symposium on High-Performance Comp ArchitectureDate of Conference: 25-29 Feb. 2012 Date Added to IEEE Xplore: 15 March 2012 ISBN Information: ISSN Information: INSPEC Accession Number: 12616335 DOI: 10.1109/HPCA.2012.6169043Publisher: IEEEConference Location: New Orleans, LA, USA Advertisement Contents SECTION 1IntroductionA fundamental practice for researchers, engineers and information services is to compare the performance of two architectures/computers using a set of benchmarks. As trivial as this task may seem, it is well known to be fraught with obstacles, especially the selection of benchmarks [31], [33], [3] and performance variability [1]. In this paper, we focus on the issue of performance variability. The variability can have several origins, such as non-deterministic architecture+software behavior [6], [26], performance measurement bias [27], or even applications themselves. Whatever the origin, the performance variability is well known to severely hamper the comparison of computers. For instance, the geometric mean performance speedups, over an initial baseline run, of 10 subsequent runs of SPLASH-2 on a commodity computer (Linux OS, 4-core 8-thread Intel i7 920 with 6 GB DDR2 RAM) are 0.94, 0.98, 1.03, 0.99, 1.02, 1.03, 0.99, 1.10, 0.98, 1.01. Even when two computers/architectures may have fairly different performance, such variability can still make the quantitative comparison (e.g., estimating the performance speedup of one computer over another) confusing or plain impossible. Incorrect comparisons can, in turn, affect research or acquisition decisions, so the consequences are significant.The most common and intuitive way of addressing the impact of variability is to compute the confidence of performance comparisons. The most broadly used techniques for estimating the confidence are parametric confidence estimate techniques [1], [27]. However, at least two kinds of wrongful practices commonly plague the usage of such techniques, potentially biasing performance comparisons, and consequently, sometimes leading to incorrect decisions.T neously based on the distribution of performance measurements (with the implicit assumption that it obeys the normal law), instead of the distribution of the sa ple mean. That wrongful practice could still lead to a correct estimate of the confidence if the distribution of performance measurements is, by chance, normal, but we will empirically show that the distribution of performance measurements can easily happen to be non-normal, so that such erroneous usage of the confidence estimate is harmful.The second issue is that, even if parametric confidence estimate techniques are correctly based on the distribution of the sample mean, the number of measurements collected is usually insufficient to achieve a normally distributed sample mean. In day-to-day practices in computer architecture research, such techniques are commonly applied when about 30 performance measurements have been collected. However, we will empirically show that a very large number of performance measurements, on the order of 160 to 240, are required for the CLT to be applicable, so that current practices can again lead to harmful usage of the parametric techniques.In this paper, we introduce the Hierarchical Performance Test (HPT) which can correctly quantify the confidence of a performance comparison even if only a few performance measurements are available. The key insight is to use nonparametric Statistic Hypothesis Tests (SHTs). Parametric statistical methods (such as the paired t-test or confidence interval) rely on some distribution assumptions, while the non-parametric SHTs quantify the confidence using data rankings. Non-parametric SHTs can work when there are only a few performance measurements, because they do not need to characterize any specific distribution. To help disseminate the use of non-parametric SHTs, we design a selfcontained Hierarchical Performance Test (HPT) and the associated software implementation, which will be openly distributed.In summary, the contributions of this paper are the following. First, we empirically highlight that traditional performance comparisons simply based on means of performance measurements can be unreliable because of the variability of performance results. As a result, we stress that every performance comparison should come with a confidence estimate in order to judge whether a comparison result corresponds to a stochastic effect or whether it is significant enough. Second, we provide quantitative evidence that two common wrongful practices, i.e., using the distribution of performance measurements instead of the distribution of sample mean, and using too few performance measurements, are either harmful or render parametric confidence techniques impractical for our domain. Third, we propose and implement the HPT framework based on nonparametric SHTs, which can provide a sound quantitative estimate of the confidence of a comparison, independently of the distribution and the number of performance measurements.The rest of the paper is organized as follows. Section 2 motivates the need for systematically assessing the confidence of performance measurements, and for using the appropriate statistical tools to do so. Section 3 investigates the impact of using the wrong distribution or of collecting an insufficient number of performance measurements for the normality assumption of parametric confidence techniques. Section 4 introduces the non-parametric hierarchical performance testing framework. Section 5 empirically compares the HPT with traditional performance comparison techniques. Section 6 reviews the related work. SECTION 2MotivationIn this section, we introduce two examples to motivate this study. The first example shows the significance of using statistical techniques in performance comparisons, and the second example shows the importance of selecting appropriate statistical techniques.In a quantitative performance comparison, the performance speedup of one computer over another is traditionally obtained by comparing the geometric mean performance of one computer (over different benchmarks) with that of another, a practice adopted by SPEC.org [31]. Following this methodology, the performance speedup of PowerEdge T710 over Xserve on SPEC CPU2006, estimated by he data collected from SPEC.org [31], is 3.50. However, after checking this performance speedup with the HPT proposed in this paper, we found that the confidence of such a performance speedup is only 0.31, which is rather unreliable (Ã¢Â‰Â¥0.95 is the statistically acceptable level of confidence). In fact, our HPT reports that the reliable performance speedup is 2.24 (with a confidence of 0.95), implying that the conclusion made by comparing geometric mean performance of two computers brings an error of 56.2\% for the quantitative comparison.Let us now use another example to briefly illustrate why using parametric confidence techniques incorrectly can be harmful. In this section we emulate the wrongful practice of computing the confidence based on the distribution of performance measurements, instead of the distribution of sample mean. We consider the quantitative performance comparison of another pair of commodity computers, Asus P5E3 Premium and CELSIUS R550. In Figure 1, we show the histograms of the SPEC ratios of the two computers (collected from SPEC.org). Clearly, neither of the histograms appears to correspond to a normal distribution. We further evaluated the normality of the distributions using the Lilliefors test (Kolmogorov-Smirnov test) [24], and we found that, for both computers, the normality assumption is indeed incorrect for confidences larger than 0.95. Fig 1. Histograms of SPEC ratios of two commodity computers.View AllVoluntarily ignoring that observation, we incorrectly use the confidence interval based on the non-normal distribution of performance measurements for the quantitative comparison. The confidence interval technique says that Asus P5E3 Premium is more than 1.02 times faster than CELSIUS R550 with a confidence of 0.95, and 1.15 times faster with a confidence of 0.12. According to the HPT, the Asus P5E3 Premium is actually more than 1.15 times faster than the CELSIUS R550 with a confidence of 0.95. So the confidence of the claim Ã¢Â€ÂœAsus P5E3 Premium is 1.15 times faster than CELSIUS R550Ã¢Â€Â is drastically under-evaluated (0.12 instead of 0.95), or conversely, the assessment of the speedup for a fixed confidence of 0.95 is again largely under-evaluated (1.02 instead of 1.15).In summary, it is important not only to assess the confidence of a performance comparison, but also to correctly evaluate this confidence. SECTION 3Issues with Current Performance Comparison PracticesIn this section, we empirically highlight that the distribution of performance measurements may not be normal, and thus it cannot be used to directly compute the confidence. Then, we empirically again show that a large number of measurements are required in order to use the Central Limit Theorem (CLT).3.1 Checking the Normality of Performance MeasurementsTo compare the performance of different computers, we expect that the performance score on each benchmark can stably reflect the exact performance of each computer. However, the performance of a computer on every benchmark is influenced by not only architecture factors (e.g., out-of-order execution, branch prediction, and chip multiprocessor [34]) but also program factors (e.g., data race, synchronization, and contention of shared resources [2]). In the presence of these factors, the performance score of a computer on a benchmark is usually a random variable [1]. For example, according to our experiments using SPLASH-2 [33], the execution time of one run of a benchmark can be up to 1.27 times that of another run of the same benchmark on the same computer. Therefore, it is necessary to provide some estimate of the confidence of a set of computer performance measurements.As mentioned before, the confidence is sometimes incorrectly assessed based on the distribution of performance measurements, which can only lead to a correct result if that distribution is normal. We empirically show that this property is not valid for the following set of rather typical performance measurements. In our experiments, we run both single-threaded (Equake, SPEC CPU2000 [31]) and multi-threaded benchmarks (Raytrace, SPLASH-2 [33] and Swaptions, PARSEC [3]) on a commodity Linux workstation with a 4-core 8-thread CPU (Intel i7 920) and 6 GB DDR2 RAM. Each benchmark is repeatedly run for 10000 times, respectively. At each run, Equake uses the Ã¢Â€ÂœtestÃ¢Â€Â input defined by SPEC CPU2000, Raytrace uses the largest input given by SPLASH-2 (car.env), and Swaptions uses the second largest input of PARSEC (simlarge). Without losing any generality, we define the performance score to be the execution time.To check the normality of the performance, we empirically study whether the normal Probability Density Function (PDF) obtained by assuming the normality of the execution time complies with the real PDF of the execution time. In our experiments, we utilize two statistical techniques, Naive Normality Fitting (NNF) and Kernel Parzen Window (KPW) [28]. The NNF technique assumes that the execution time obeys a normal law and estimates the corresponding normal distribution, while the KPW technique provides the real distribution of the execution time without assuming a normal distribution. If the normal distribution obtained by the NNF complies with the real distribution estimated by the KPW, then the performance score obeys a normal law. Otherwise, it does not. Statistically, the NNF technique directly employs the mean and deviation of the sample (with 10000 measurements of the execution time) as the mean and standard deviation of the normal distribution. In contrast, without assuming the normality, the KPW technique estimates the real distribution of the execution time in a Monte-Carlo style. It estimates directly the probability density at each point via histogram construction and Gaussian kernel smoothing. By comparing the PDFs obtained by the two techniques, we can easily identify whether or not the distribution of performance measurements obeys a normal law.According to the experimental results illustrated in Figure 2, the normality does not hold for the performance score of the computer on all three benchmarks, as evidenced by the remarkably long right tails and short left tails of the estimated performance distributions for Equake, Raytrace and Swaptions. Such observations are surprising but not counter-intuitive due to the intrinsic non-determinism of computers and applications. Briefly, it is hard for a program to execute faster than a threshold, but easy to be slowed down by various events, especially for multi-threaded programs which are affected by data races, thread scheduling, synchronization order, and contentions of shared resources. Figure 2. Estimating Probability Density Functions (PDFs) on Equake (SPEC CPU2000), Raytrace (SPLASH-2) and Swaptions (PARSEC) by KPW (black curves above the grey areas) and NNF (black curves above the white areas), from 10000 repeated runs of each benchmark on the same computer.View AllAs a follow-up experiment, we use a more rigorous statistical technique to study whether the execution times of the 27 benchmarks of SPLASH-2 and PARSEC (using Ã¢Â€ÂœsimlargeÃ¢Â€Â inputs) distribute normally; each benchmark is repeatedly run on the commodity computer for 10000 times again. Based on these measurements, the Lilliefors test (Kolmogorov-Smirnov test) [24] is utilized to estimate the confidence that the execution time does not obey the normal law, i.e., the confidence that the normality assumption is incorrect. Interestingly, it is observed that for every benchmark of SPLASH-2 and PARSEC, the confidence that the normality assumption is incorrect is above 0.95. Our observation with SPLASH-2 and PARSEC is significantly different from the observation of Georges et al. [12] that single-benchmark performance on single cores (using SPECjvm98) distributes normally, suggesting that the performance variability of multi-threaded programs is fairly different from that of single-threaded programs.In fact, the same can be observed for the performance variability of a computer over a set of different benchmarks. Considering the performance score of a computer which may vary from one benchmark to a other, we empirically study whether the distribution of the performance score obeys the normality law. Figure 3 illustrates the normal probability plot [5] for the performance of a commodity computer (4-Core Intel Core i7Ã¢Â€Â“870, Intel DP55KG motherboard), where the data is collected from the SPEC online repository [31]. In each probability plot presented in Figure 3, if the curve matches well the straight line, then the performance distributes normally over the corresponding benchmark suite; if the curve departs from the straight line, then the performance does not distribute normally. Obviously, neither of the figures shows a good match between the curve and straight line, implying that the performance of the computer does not distribute normally over both SPECint2006 and SPECfp2006. Figure 3. Graphically assessing whether the performance measurements of a commodity computer distribute normally (normal probability plots [5]) using performance scores (SPEC ratios) for SPECint2006 and SPECfp2006.View AllFinally, we use the Lilliefors test [24] to analyze the data of 20 other commodity computers randomly selected from the SPEC online repository [31]. Using the whole SPEC CPU2006 suite, all 20 computers exhibit non-normal performance with a confidence larger than 0.95. For SPECint2006, 19 out of 20 computers exhibit non-normal performance with a confidence larger than 0.95, and for SPECfp2006, 18 out of 20 computers exhibit non-normal performance with a confidence larger than 0.95.So we can consider that, in general, the distribution of performance measurements does not obey the normal law, and thus, it should never be used to directly estimate the confidence of these measurements with parametric techniques. Only the distribution of the sample mean should be used, as stated by the Central Limit Theorem.3.2 Checking the Applicability of the Central Limit TheoremSo far we have empirically shown that the performance of computers cannot be universally characterized by normal distributions. However, it is still possible to obtain a normally distributed mean of performance measurements (in order to apply parametric techniques) given a sufficiently large number of measurements, as guaranteed by the Central Limit Theorem. More specifically, let \{x1,x2,Ã¢Â€Â¦,xn\} be a size- n sample consisting of n measurements of the same non-normal distribution with mean ÃŽÂ¼ and finite variance ÃÂƒ2, and Sn=(Ã¢ÂˆÂ‘ni=1xi)/n be the mean of the measurements (i.e., sample mean). According to the classical version of the CLT contributed by Lindeberg and LÃƒÂ©vy [4], when the sample size n is sufficiently-large, the sample mean approximately obeys a normal distribution. Nevertheless, in practice, it is unclear how large the sample size should be to address the requirement of Ã¢Â€Âœa sufficiently large sampleÃ¢Â€Â. In this section, we empirically show that the appropriate sample size for applying the CLT is usually too large to be compatible with current practices in computer performance measurements.In order to obtain a large number performance measurements, we use the KDataSets [7]. A notable feature of KDataSets is that it provides 1000 distinct data sets for each of 32 different benchmarks (MiBench [14]), for a total of 32,000 distinct runs. We collect the detailed performance scores (performance ratios normalized to an ideal processor executing one instruction per cycle) of a Linux workstation (with 3GHz Intel Xeon dualcore processor, 2GB RAM) over the 32,000 different combinations of benchmarks and data sets of KDataSets [7]. In each trial, we fix the number of different samples to 150, leading to 150 observations of sample mean, which are enough to obtain a normal distribution as predicted by the CLT [15]. In order to construct each sample, we randomly select n performance scores out of the 32,000 performance scores, where the sample size n (i.e., number of measurements) is varied from 10 to 280 by increments of 20. For each trial with a fixed sample size, we estimate the probability distribution of the sample mean via the statistical techn que called Kernel Parzen Window (KPW) [28]. Technically, the KPW has a parameter called Ã¢Â€Âœwindow sizeÃ¢Â€Â or Ã¢Â€Âœsmoothing bandwidthÃ¢Â€Â, which determines how KPW will smooth the distribution curve. In general, the larger the window size, the smoother the curve. But if we choose a too large window size, there is a risk that a non-normal curve is rendered as normal by KPW. In order to avoid that, in each trial we start from a small window size, and we increase it until the distribution curve becomes smooth enough. After that, we judge whether the distribution has approached a normal distribution by checking if it is symmetric with respect to its center. The different probability distributions over the 15 trials are illustrated in Figure 4. Figure 4. How many performance measurements are required to construct a sufficiently-large sample?View AllClearly, the sample mean does not distribute normally given a small (e.g., n=10Ã¢ÂˆÂ’140) sample size. When the sample size becomes larger than 240, the distribution of the mean performance seems to be a promising approximation of a normal distribution. In addition, we further carry out the Lilliefors test for each trial, and we find that the mean performance does not distribute normally when n{\textless}160. The above observation implies that at least 160 to 240 performance measurements are necessary to make the CLT and parametric techniques applicable, at least for this benchmark suite. However, such a large number of performance measurements can rarely be collected in day-to-day practices; most computer architecture research studies rely on a few tens of benchmarks, with one to a few data sets each, i.e., far less than the aforementioned number of required performance measurements. In order to cope with a small number of performance measurements, we propose to use a non-parametric statistical framework. SECTION 4Non-parametric Hierarchical Performance Testing FrameworkStatistical inference techniques are popular for decision making processes based on experimental data. One crucial branch of statistical inference is called Statistical Hypothesis Test (i.e., SHT).More specifically, an SHT is a procedure that makes choices between two opposite hypotheses (propositions), the NULL (default) hypothesis and the alternative hypothesis. The NULL hypothesis represents the default belief, i.e., our belief before observing any evidence, and the alternative hypothesis (often the claim we want to make) is a belief opposite to the NULL hypothesis. In performance comparison, a typical NULL hypothesis may be Ã¢Â€Âœcomputer A is as fast as computer BÃ¢Â€Â, and a typical alternative hypothesis may be Ã¢Â€Âœcomputer Ais faster than computer BÃ¢Â€Â. At the beginning of an SHT, one assumes the NULL hypothesis to be correct, and constructs a statistic (say, Z) whose value4.1 General FlowConcretely, the HPT employs Wilcoxon Rank-Sum Test [35] to check whether the difference between the performance scores of two computers on each benchmark is significant enough (i.e., the corresponding significance level isAmong the most famous and broadly used SHTs, many are parametric ones which rely on normally distributed means of measurements. In the rest of this section, we introduce a Hierarchical Performance Testing (HPT) framework, which integrates non-parametric SHTs for performance comparisons. can be calculated from the observed data. The value of Z determines the possibility that the NULL hypothesis holds, which is critical for making a choice between the NULL hypothesis and the alternative hypothesis. The possibility that the NULL hypothesis does hold is quantified as the socalled p-value (or significance probability) [21], which is a real value between 0 and 1 that can simply be considered as a measure of risk associated with the alternative hypothesis. The p-value is an indicator for decision-making: when the p-value is small enough, then the risk of incorrectly rejecting the NULL hypothesis is very small, and the confidence of the alternative hypothesis (i.e., 1 Ã¢Â€Â“ p-value) is large enough. For exa ple, in an SHT, when the p-value of the NULL hypothesis Ã¢Â€Âœcomputer A is as fast as computer BÃ¢Â€Â is 0.048, we only have a 4.8\% chance of rejecting the NULL hypothesis when it actually holds. In other words, the alternative hypothesis Ã¢Â€Âœcomputer A is faster than computer BÃ¢Â€Âhas confidence 1Ã¢ÂˆÂ’0.048=0.952. Closely related to the p-value, the significance level acts as a scale of the ruler for the p-value (frequently-used scales include 0.001, 0.01, 0.05, and 0.1). A significance level ÃŽÂ±Ã¢ÂˆÂˆ[0,1], can simply be viewed as the confidence level 1Ã¢ÂˆÂ’ÃŽÂ±. As a statistical convention, a confidence no smaller than 0.95 is often necessary for reaching the final conclusion. small enough), in other words, whether the observed superiority of one computer over another is reliable enough. Only significant (reliable) differences, identified by the SHTs in single-benchmark comparisons, can be taken into account by the comparison over different benchmarks, while those insignificant differences will be ignored (i.e., the insignificant differences are set to 0) in the comparison over different benchmarks. Based on single-benchmark performance measurements, the Wilcoxon Signed-Rank Test [9], [35] is employed to statistically compare the general performance of two computers. Through these non-parametric SHTs, the HPT can quantify the confidence for performance comparisons. In this section, the technical details of the HPT will be introduced.1Let us assume that we are comparing two computers A and B over a benchmark suite consisting of n benchmarks. Each computer repeatedly runs each benchmark m times (mÃ¢Â‰Â¥3). Let the performance scores of A and B at their j-th runs on the i-th benchmark be ai,j and bi,j respectively.Then the performance samples of the computers can be represented by performance matrices SA=[ai,j]nÃƒÂ—m and SB=[bi,j]nÃƒÂ—m respectively. For the corresponding rows of SA and SB (e.g., the ÃÂ„-th rows of the matrices,ÃÂ„=1.Ã¢Â€Â¦,n), we carry out the Wilcoxon Rank-Sum Test to investigate whether the difference between the performance scores of A and B is significant enough. The concrete steps of Wilcoxon Rank-Sum Test are the following:Let the NULL hypothesis of the SHT Ã¢Â€ÂœHÃÂ„,0the performance scores of A and B on the ÃÂ„-th benchmark are equivalent to each otherÃ¢Â€Â; let the alternative hypothesis of the SHT be Ã¢Â€ÂHÃÂ„,1the performance score of A is higher than that of B on the ÃÂ„ -th benchmarkÃ¢Â€Â or Ã¢Â€ÂœHÃÂ„,2: the performance score of B is higher than that of A on the ÃÂ„-th benchmarkÃ¢Â€Â, depending on the motivation of carrying out the SHT. Define the significance level be ÃŽÂ±ÃÂ„; we suggest setting ÃŽÂ±ÃÂ„=0.05 for mÃ¢Â‰Â¥5 and 0.10 for the rest cases.Sort ÃŽÂ±ÃÂ„,1,aÃÂ„,2,Ã¢Â€Â¦,aÃÂ„,m,bÃÂ„,1,bÃÂ„,2,Ã¢Â€Â¦,bÃÂ„, m in ascending order, and assign each of the scores the corresponding rank(from 1 to 2m). In case two or more scores are the same, but their original ranks are different, then renew the ranks by assigning them the average of their original ranks2. Afterwards, for A and B, we can define their rank sums (on the ÃÂ„ -th benchmark) to be: Ra,ÃÂ„=Ã¢ÂˆÂ‘j=1mRankÃÂ„(aÃÂ„,j),Rb,ÃÂ„=Ã¢ÂˆÂ‘j=1mRankÃÂ„(bÃÂ„,j),View Source\{{\textbackslash}rm R\}\_\{a,{\textbackslash}tau\}={\textbackslash}displaystyle{\textbackslash}sum\_\{j=1\}{\textasciicircum}\{m\}\{{\textbackslash}rm Rank\}\_\{{\textbackslash}tau\}(a\_\{{\textbackslash}tau,j\}),{\textbackslash}quad \{{\textbackslash}rm R\}\_\{b,{\textbackslash}tau\}={\textbackslash}displaystyle{\textbackslash}sum\_\{j=1\}{\textasciicircum}\{m\}\{{\textbackslash}rm Rank\}\_\{{\textbackslash}tau\}(b\_\{{\textbackslash}tau,j\}), where RankÃÂ„(Ã¢Â‹ ) provides the rank of a performance score on the ÃÂ„ -th benchmark.Case [m{\textless}12]3: When the alternative hypothesis of the SHT is HÃÂ„,1 we reject the NULL hypothesis and accept HÃÂ„,1 if Ra,ÃÂ„ is no smaller than the critical value (right tail, Wilcoxon Rank-Sum Test) under the significance level ÃŽÂ±ÃÂ„. When the alternative hypothesis of the SHT is HÃÂ„,2, we reject the NULL hypothesis and accept HÃÂ„,2 if Rb,ÃÂ„ is nosmaller than the critical value under the significance level ÃŽÂ±[15].Case [mÃ¢Â‰Â¥12]: Define two new statistics za,ÃÂ„ and zb,ÃÂ„ as follows:za,ÃÂ„=Ra,ÃÂ„Ã¢ÂˆÂ’12m(2m+1)112m2(2m+1)Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂš,zb,ÃÂ„=Rb,rÃ¢ÂˆÂ’12m(2m+1)112m2(2m+1)Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂš.View Sourcez\_\{a,{\textbackslash}tau\}=\{\{{\textbackslash}rm R\}\_\{a,{\textbackslash}tau\}-\{1{\textbackslash}over 2\}m(2m+1){\textbackslash}over {\textbackslash}sqrt\{\{1{\textbackslash}over 12\}m{\textasciicircum}\{2\}(2m+1)\}\},{\textbackslash}quad z\_\{b,{\textbackslash}tau\}=\{\{{\textbackslash}rm R\}\_\{b,r\}-\{1{\textbackslash}over 2\}m(2m+1){\textbackslash}over {\textbackslash}sqrt\{ \{1{\textbackslash}over 12\}m{\textasciicircum}\{2\}(2m+1)\}\}.Under the NULL hypothesis, za,ÃÂ„ and zb,ÃÂ„ approximately obey the standard normal distribution N(0,1). When the alternative hypothesis of the SHT is HÃÂ„,1, we reject the NULL hypothesis and accept HÃÂ„,1 if za,ÃÂ„ is no smaller than the critical value (right tail, standard normal distribution) under the significance level ÃŽÂ±; when the alternative hypothesis of the SHT is HÃÂ„,2, we reject the NULL hypothesis and accept HÃÂ„,2 if zb,ÃÂ„ is no smaller than the critical value under the significance level ÃŽÂ±[15].After carrying out the above SHT with respect to theÃÂ„ -th benchmark (ÃÂ„=1,Ã¢Â€Â¦,n), we are able to assign the difference (denoted by dÃÂ„) between the performance of A and B Concretely, if the SHT accepts HÃÂ„,1 or HÃÂ„,2 with a promising significance level (e.g., 0.01 or 0.05), then we let dÃÂ„=median\{ÃŽÂ±ÃÂ„,1,aÃÂ„,2,Ã¢Â€Â¦,aÃÂ„,m\} Ã¢ÂˆÂ’median\{bÃÂ„,1,bÃÂ„,2,Ã¢Â€Â¦,bÃÂ„,m\},ÃÂ„=1,Ã¢Â€Â¦,n.View Source{\textbackslash}eqalignno\{ d\_\{{\textbackslash}tau\}\&= \{{\textbackslash}rm median\} {\textbackslash}\{{\textbackslash}alpha\_\{{\textbackslash}tau,1\}, a\_\{{\textbackslash}tau,2\}, {\textbackslash}ldots, a\_\{{\textbackslash}tau,m\}{\textbackslash}\}{\textbackslash}cr \&{\textbackslash} {\textbackslash} {\textbackslash}, {\textbackslash}!{\textbackslash}! -\{{\textbackslash}rm median\} {\textbackslash}\{b\_\{{\textbackslash}tau,1\}, b\_\{{\textbackslash}tau,2\}, {\textbackslash}ldots, b\_\{{\textbackslash}tau,m\}{\textbackslash}\},{\textbackslash}quad {\textbackslash}tau=1, {\textbackslash}ldots, n.\} Otherwise (if the NULL hypothesis HÃÂ„,0 has not been rejected at a promising significant level), we let dÃÂ„=0, i.e., we ignore the insignificant difference between the performance scores of A and B. d1,d2,Ã¢Â€Â¦,dn will then be utilized in the following Wilcoxon Signed-Rank Test for the performance comparison over different benchmarks: Let the NULL hypothesis of the SHT be Ã¢Â€ÂœH0the general performance of Ais equivalent to that B; let the alternative hypothesis of the SHT beH1the general performanceof Ais better than that of B or Ã¢Â€Âœ H2: the general performance of Bis better than that of AÃ¢Â€Â, depending on the motivation of carrying out the SHT.Rank d1,d2,Ã¢Â€Â¦,dn according to an ascending order of their absolute values. In case two or more absolute values are the same, then renew the ranks by assigning them the average of their original ranks. Afterwards, for A and B, we can define their signed-rank sums be: RARB=Ã¢ÂˆÂ‘i:di{\textgreater}0Rank(di)+12Ã¢ÂˆÂ‘i:di=0Rank(di),=Ã¢ÂˆÂ‘i:di{\textless}0Rank(di)+12Ã¢ÂˆÂ‘i:di=0Rank(di),View Source{\textbackslash}eqalignno\{ \{{\textbackslash}rm R\}\_\{A\}\&={\textbackslash}displaystyle{\textbackslash}sum\_\{i:d\_\{i\}{\textgreater}0\} \{{\textbackslash}rm Rank\} (d\_\{i\})+\{1{\textbackslash}over 2\}{\textbackslash}sum\_\{i:d\_\{i\}=0\} \{{\textbackslash}rm Rank\} (d\_\{i\}),{\textbackslash}cr \{{\textbackslash}rm R\}\_\{B\}\&={\textbackslash}displaystyle{\textbackslash}sum\_\{i:d\_\{i\}{\textless} 0\} \{{\textbackslash}rm Rank\} (d\_\{i\})+\{1{\textbackslash}over 2\}{\textbackslash}sum\_\{i:d\_\{i\}=0\}\{{\textbackslash}rm Rank\}(d\_\{i\}),\} where Rank (di) provides the rank of the absolute value of di, which was described above.Case [n{\textless}25]4: When the alternative hypothesis of the SHT is H1, we reject the NULL hypothesis and accept H1 if RB is no larger than the critical value (one-side Wilcoxon Signed-Rank Test) under the significance level ÃŽÂ± When the alternative hypothesis of the SHT is H2, we reject the NULL hypothesis and accept H2 if RA is no larger than the critical value under the significance level ÃŽÂ± The critical values of Wilcoxon Signed-Rank Test are available in statistics books [15].Case [nÃ¢Â‰Â¥25]: Define two new statistics zA and zB as follows: zA=RAÃ¢ÂˆÂ’14n(n+1)124n(n+1)(2n+1)Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂš,zB=RBÃ¢ÂˆÂ’14n(n+1)124n(n+1)(2n+1)Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂ’Ã¢ÂˆÂš.View Sourcez\_\{A\}=\{\{{\textbackslash}rm R\}\_\{A\}-\{1{\textbackslash}over 4\}n(n+1){\textbackslash}over {\textbackslash}sqrt\{\{1{\textbackslash}over 24\}n(n+1)(2n+1)\}\}, {\textbackslash}, z\_\{B\}=\{\{{\textbackslash}rm R\}\_\{\{ B\}\}-\{1{\textbackslash}over 4\}n(n+1){\textbackslash}over {\textbackslash}sqrt\{\{1{\textbackslash}over 24\}n(n+1)(2n+1)\}\}. Under the NULL hypothesis, zA and zB approximately obey the standard normal distribution N(0,1). Hence, when the alternative hypothesis of the SHT is H1, we reject the NULL hypothesis and accept H1. if zB is no larger than the critical value (lower tail, standard normal distribution) under the signifi ance level ÃŽÂ± when the alternative hypothesis of the SHT is H2:, we reject the NULL hypothesis and accept H2 if zA is no larger than the critical ÃÂ„ value under the significance level ÃŽÂ±, For the comparison over different benchmarks, the outputs of the HPT, including the comparison result and its confidence, are finally presented by the above Wilcoxon Signed-Rank Test. For-mally, given a fixed significance level (ÃŽÂ± for the HPT, we utilize Confidence (HPT: SAÃ¢Â‰Â»SB)Ã¢Â‰Â¥r to represent the following conclusion made by the HPT: Ã¢Â€ÂœA outperforms B with the confidence rÃ¢Â€Â, where r=1Ã¢ÂˆÂ’ÃŽÂ±.4.2 Quantitative Comparison: Statistical Speedup TestingSo far we have shown how to carry out qualitative performance comparison with the HPT. In addition to the qualitative comparison, in most cases we are more interested in quantitative comparison results such as Ã¢Â€ÂœComputer A is more than ÃŽÂ³ times faster than Computer BÃ¢Â€Â, where ÃŽÂ³Ã¢Â‰Â¥1 is defined as the speedup-under-test. Traditionally, such kind of arguments are often obtained directly by comparing the means of performance scores with respect to computers A and B. Taking the SPEC convention as an example, if the mean (geometric) SPEC ratios of A is ten times that of B, then one would probably conclude that Ã¢Â€ÂœA is ten times faster than BÃ¢Â€Â. Such a quantitative comparison is dangerous since we do not know how much we can trust the result. Fortunately, the HPT framework offers two solutions for tackling speedup arguments. The first solution requires us to specify the concrete value of ÃŽÂ³ before the test. Afterwards, we shrink the performance scores of computer A by transforming the corresponding performance matrix SA to SA/ÃŽÂ³ (without losing generality, we employ the normalized performance ratio as the performance score with respect to each benchmark, where a larger performance score means better performance). Considering a virtual computer with performance matrix SA/ÃŽÂ³, if the HPT framework states that the virtual computer outperforms computer B with confidence r, then we can claimÃ¢Â€Â A is more than ÃŽÂ³ times faster than B with confidence rÃ¢Â€Â. In general, if we specify a more (less) conservative speedup ÃŽÂ³before speedup testing, the corresponding speedup argument will have a larger (smaller) confidence r. Users should keep a balance between the speedup and the corresponding confidence, so as to make a convincing yet not-too-conservative conclusion.In many cases, instead of deciding a speedup ÃŽÂ³ before the statistical test, one would like to know the largest speedup that results in a reliable comparison result (for a given confidence r). To address this need, our HPT framework also offers an alternative way of estimating the speedup and corresponding confidence. Guided by the above notion, we formally define the r-Speedup (computer A over computer B,r -Speedup (A,B)) to besup\{ÃŽÂ³Ã¢Â‰Â¥1;confidence(HPT:1ÃŽÂ³SAÃ¢Â‰Â»SB)Ã¢Â‰Â¥r\}.View Source{\textbackslash}hbox\{sup\}{\textbackslash}, {\textbackslash}left{\textbackslash}\{{\textbackslash}gamma{\textbackslash}geq 1;confidence{\textbackslash}left(\{{\textbackslash}rm HPT\} : \{1{\textbackslash}over {\textbackslash}gamma\}S\_\{A\}{\textbackslash}succ S\_\{B\}{\textbackslash}right){\textbackslash}geq r{\textbackslash}right{\textbackslash}\}.To be specific, the r -Speedup of computer A over computer B is the largest speedup of A over B for confidence r. In practice, we can restrict the precision (e.g., 2 decimals) when estimating the r -Speedup via heuristic optimization techniques. After predefining the confidence level r (e.g., r=0.95), the r -Speedup can be viewed as a quantitative indicator of performance speedup with the guarantee of confidence r. The whole HPT framework, including the r -Speedup, has been implemented, and it will be disseminated as an open-source software [16].4.3 an Example of Quantitative Performance ComparisonIn this subsection, the quantitative performance comparison of two commodity computers, X (Linux OS, 4-core 8-thread Intel i7 920 with 6 GB DDR2 RAM) and Y (Linux OS, 8-core AMD Opteron 8220 with 64 GB DDR2 RAM) is presented as an example of applying the HPT and speedup test. In our experiments, each SPLASH-2 benchmark (8 threads) is repeatedly run 5 times on each computer, using the defau t workloads of SPLASH-2. By specifying the speedup-under-test ÃŽÂ³ to be 1.76, we use the HPT to test how reliable is the proposition Ã¢Â€ÂœComputer X is more than 1.76 times faster than Computer Y ÃƒÂ‹Ã‚Â® over SPLASH-2. Testing such a proposition is equivalent to testing Ã¢Â€ÂœComputer X{\textasciitilde} is faster than Computer Y ÃƒÂ‹Ã‚Â® over SPLASH-2, where X{\textasciitilde} is a virtual computer whose performance scores are always 1/1.76 of the corresponding scores of the real computer X. Table 1 presents the details of the comparison. To be specific, all performance scores are normalized to the first run of computer Y on each benchmark. In order to conduct a quantitative comparison, we divide all performance scores of X by 1.76 times (we store these reduced scores in SX{\textasciitilde}), and utilize the HPT to compare the reduced scores against those of computer Y (stored in SY). For the ÃÂ„th benchmark (ÃÂ„=1,Ã¢Â€Â¦,n), Ã¢Â€ÂœStat. Win.Ã¢Â€Â indicates the winner whose performance on the ÃÂ„th benchmark is significantly (with confidence 0.95) better. We indicate Ã¢Â€ÂœX{\textasciitilde} ÃƒÂ‹Ã‚Â® if the reduced performance of X still wins, and we indicate Ã¢Â€ÂY ÃƒÂ‹Ã‚Â® if the performance of Y wins over the reduced performance of X. In case there is no definite winner, we indicate Ã¢Â€ÂœTieÃ¢Â€Â. Ã¢Â€ÂœMed.Ã¢Â€Â indicates the median of the five performance scores (of A and B), Ã¢Â€ÂœDiff.Ã¢Â€Â shows the (significant) difference between the median performance scores of A and B, Ã¢Â€ÂœRankÃ¢Â€Â shows the rank of the absolute value of dr. According to the HPT, the virtual computer X{\textasciitilde} beats computer Y significantly on 8 benchmarks, ties on 2 benchmarks, loses on 4 benchmarks. Following the flow introduced in Section 4.1, the proposed HPT concludes that Ã¢Â€ÂœComputer X{\textasciitilde} is faster than Computer Y with confidence 0.95Ã¢Â€Â, suggesting that Ã¢Â€ÂœComputer X is more than 1.76 times faster than Computer Y with confidence 0.95Ã¢Â€Â (i.e., the 0.95-Speedup of Computer X over Computer Y is 1.76 over all SPLASH -2 benchmarks), where 0.95 is the statistically acceptable level of confidence. Table 1. Statistical quantitative comparison of computers X and Y over SPLASH-2 (Speedup: 1.76) SECTION 5Experimental Comparisons5.1 Comparisons Using the Geometric Mean PerformanceIn traditional quantitative comparisons, the Geometric Mean (GM) of the performance scores of a computer over different benchmarks is often utilized to estimate the performance speedup of one computer over another. In most cases, such comparison results, presented without confidence estimates, are often unreliable. Taking the performance comparison between computers X and Y presented in Section 4.3 as an example, the GM-speedup (the performance speedup obtained by comparing the geometric mean performance score) of computer X over computer Y is 2.14. The corresponding confidence of this performance speedup (estimated by the proposed HPT) is 0.64, which is far less than the acceptable level 0.95. We then perform the following more extensive experiments: we collect the (SPEC CPU2006) performance reports of 14 different computers from SPEC.org [31], and analyze both the 0.95-Speedup (performance speedup estimated by the HPT, with the guaranteed confidence 0.95) and GM-Speedup of one computer over another with the proposed HPT. Table 2 presents the performance speedups and the corresponding confidences over 7 pairs of computers5, It can be observed from Table 2 that the GM-Speedup is higher than the 0.95-Speedup on all 7 pairs of computers, and the largest error between the GM-Speedup and 0.95-Speedup can be 56.3\%. Meanwhile, compared with the acceptable confidence 0.95, the loss of confidence brought by the unreliable GM-Speedup ranges from 28.4\% to 87.4\%, showing that all 7 GM-Speedups are rather unreliable.5.2 Comparisons Using Parametric TechniquesIn this subsection, the proposed non-parametric HPT is compared against two parametric statistical techniques (confidence interval and paired t -test) requiring normally distributed sample mean. Ideally, a fair comparison between a non-parametric technique and a parametric technique requires that both the non-parametric and parametric techniques are built upon the same statistic, the mean of the measurements. However, using this statistic in a nonparametric manner requires more performance measurements, e.g., several hundred different machines, and more sophisticated statistical techniques such as the permutation test or bootstrap. Due to the lack of available performance data on SPEC.org, we leave this improved comparison for future work.Let us first recall the example presented in Section 4.3. Using the assertion Ã¢Â€ÂœComputer X is more than 1.76 times faster than Computer Y ÃƒÂ‹Ã‚Â®, we compare the effectiveness of HPT against that of the parametric techniques. Technically, the effectiveness of each technique (HPT, paired t -test and confidence interval) can be statistically measured by the Ã¢Â€Âœstatistical powerÃ¢Â€Â of the technique [9], [11], which is a broadly adopted criterion for comparing the effectiveness of SHTs. The power of a statistical technique for testing the confidence of a hypothesis can be approximately estimated by the probability of accepting the alternative hypothesis [9]. For the comparison result Ã¢Â€ÂœComputer X is more than 1.76 times faster than Computer YÃ¢Â€Â, the power of each statistical technique can be estimated by carrying out 1000 repeated runs of SPLASH-2 on X and Y, and studying the average confidence of the comparison result over 1000 repeated statistical tests using the same technique. According to our experiments, the power of HPT, estimated over 1000 repeated runs of SPLASH-2 on computers X and Y, is 0.89, which is significantly larger than that of the confidence interval (0.67), and that of the paired t -test (0.76). Statistically, this shows that HPT significantly outperforms the paired t-test for the performance comparison of computers. Table 2. Quantitative Performance Comparisons based on SPEC CPU2006, where the 0.95-Speedups are obtained by the proposed HPT, each GM-Speedup is obtained by comparing the geometric mean SPEC ratios of the corresponding pair of computers, and all HPT-confidences are estimated by the proposed HPT.Table 3. Comparisons of confidences obtained by the HPT and parametric techniques, where the HPT-Confidences are obtained by the proposed HPT, CI-Confidences are obtained by the confidence interval, and t -Confidences are obtained by the paired t-test. According to the statistical convention, the confidences in bold are the acceptable ones (Ã¢Â‰Â¥0.95), and the rest are unacceptable ones ({\textless}0.95).Now, we carry out another experiment using the data collected from SPEC.org [31], with the goal of showing that the parametric techniques can be rather inaccurate compared to HPT. Our empirical study still involves the 7 aforementioned pairs of computers. By fixing the performance speedup for each pair of computers, Table 3 compares the confidence obtained by the HPT with those obtained by parametric techniques. We can observe that the confidence provided by the confidence interval technique is often rather inaccurate, and the largest confidence estimate error is 87.4\%. At the acceptable confidence level of 0.95, the confidence interval technique is so conservative that all 7 comparison results are incorrectly considered to be unreliable (while the HPT accepts all of them). The paired t -test performs slightly better, though it still rejects 5 out of the 7 comparison results due to the inaccurate t -Confidence. In summary, using inappropriate statistical techniques can simply result in incorrect conclusions on the validity of a performance comparison.Finally, we also compare the performance speedups obtained by parametric techniques against those obtained by HPT when the level of confidence is set to 0.95. As presented in Table 4, the speedup error with respect to the confidence interval ranges from 6.5\% to 44.7\%, while the error with respect to the t -test ranges from 1.4\% to 21.6\%. Again, these results highlight the impact of statistical techniques on the outcome of performance comparisons. Table 4. Comparisons of 0.95-perf rmance speedups obtained by the HPT and parametric techniques, where each speedup has a confidence of 0.95 for each technique. SECTION 6Related WorkTraditionally, performance comparisons of computers mainly rely upon one metric (e.g., geometric mean and harmonic mean) [8], [17], [20], [25], [30], though this approach can be rather unreliable. Having realized the importance of statistical inference, Lilja suggested to introduce several parametric statistical methods (e.g, confidence interval) to evaluate computer performance [23]. Alameldeen and Wood carried out in-depth investigations on the performance variability of multi-threaded programs, and they suggested to use the confidence interval and t -test, two parametric techniques, in order to address the issue of variability [1]. Later, in the context of Java performance evaluation, Georges et al. [12] found that single-benchmark performance (SPECjvm98) on several single-core computers can, in general, be characterized using normal distributions, and thus, they can recommend using the confidence interval technique in this case. While valid, their observation does not seem to generalize to the broader case of multicore systems and multi-threaded applications, based on our own experiments. Iqbal and JohnÃƒÂŠÃ‚Â¼s empirical study [19] generally supported the log-normality for characterizing the SPEC performance of computers. However, their experiments were conducted after removing all Ã¢Â€Âœoutlier benchmarksÃ¢Â€Â in SPEC CPU2006. They also proposed a performance ranking system [19]. But unlike the Wilcoxon test which uses rank information to construct statistics for computing confidence, the system directly offers a performance ranking without presenting the corresponding confidence.However, we observed that few computer architecture studies yet acknowledge the importance of proper confidence estimates for performance comparisons and measurements: among 521 papers surveyed at ISCA (194 papers, 2006Ã¢Â€Â“2010), HPCA (158 papers, 2006Ã¢Â€Â“2010) and MICRO (169 papers, 2006Ã¢Â€Â“2009), only 28 papers (5.4\%) resort to confidence estimates in order to assess the variability of performance measurements, among which 26 (5\%) rely upon the confidence interval technique, and only 3 (0.57\%) use the more sophisticated but still parametric t.At the same time, many other statistical techniques have already been used to cope with various issues in computer architecture research. For instance, statistical techniques were used for sampling simulation [36], principal components analysis was used to evaluate the representativeness of benchmarks [3], [29], and regression techniques were used to model the design space of processors [10], [13], [18], [22]. Therefore, the computer architecture community is already largely familiar with complex statistical tools, so that embracing a more rigorous performance measurement and comparison process is only a logical extension of the current trend. SECTION 7ConclusionWe first highlight the importance and impact of variability in performance measurements and comparisons, as well as the risk of inappropriately using current parametric confidence techniques, and the fact they require a large number of performance measurements when applied to multi-cores and multi-threaded applications, making them largely impractical.We propose a framework for achieving both a rigorous and practical comparison of computer architecture performance. In the proposed HPT, we adopt non-parametric SHTs which do not require a normal distribution of the sample mean of performance measurements, and thus, which can accommodate few such measurements. Besides the benefits for performance comparisons, we have implemented the HPT as an easy-to-use open-source software, requiring no mathematical background. AuthorsFiguresReferencesCitationsKeywordsMetricsFootnotes},
	year = {2012},
	keywords = {Metrics},
}
